{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\\\data2\\\\Arms_resized\\\\\n",
      "Full dataset tensor: (784, 20, 20)\n",
      "Mean: 0.112733\n",
      "Standard deviation: 0.132943\n",
      ".\\\\data2\\\\Figure_normal_legs_resized\\\\\n",
      "Full dataset tensor: (622, 20, 20)\n",
      "Mean: 0.0883928\n",
      "Standard deviation: 0.15396\n",
      ".\\\\data2\\\\FigureWheels_resized\\\\\n",
      "Full dataset tensor: (222, 20, 20)\n",
      "Mean: 0.0750712\n",
      "Standard deviation: 0.153478\n",
      ".\\\\data2\\\\Head_resized\\\\\n",
      "Full dataset tensor: (473, 20, 20)\n",
      "Mean: 0.0370673\n",
      "Standard deviation: 0.192848\n",
      "Total samples number: (2101, 20, 20)\n",
      "Samples for tests: 526\n",
      "Samples for trains: 1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 804       \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 241,404\n",
      "Trainable params: 241,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1181 samples, validate on 394 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.32948, saving model to ./weights_test.net\n",
      " - 1s - loss: 0.3885 - acc: 0.8366 - val_loss: 0.3295 - val_acc: 0.8503\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.32948 to 0.27015, saving model to ./weights_test.net\n",
      " - 0s - loss: 0.2676 - acc: 0.8878 - val_loss: 0.2702 - val_acc: 0.9023\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 0s - loss: 0.2208 - acc: 0.9066 - val_loss: 0.3001 - val_acc: 0.8985\n",
      "Epoch 00003: early stopping\n",
      "526/526 [==============================] - 0s 46us/step\n",
      "[0.24784458686417046, 0.90541825095057038]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQlSgRCRIBUIEQlSgRCRoP8HwWn/G1MAzDgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d46449bbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK: LOAD IMAGES FROM DATASET INTO ACTUAL CONTAINERS FOR LATER FITTING\n",
    "# testing the image collection\n",
    "# with new resized images\n",
    "# minimum number of images = 100\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "image_size = 20  # Pixel width and height\n",
    "pixel_depth = 255.0  # Number of levels per pixel\n",
    "\n",
    "folders_resized = [\n",
    "    r'.\\\\data2\\\\Arms_resized\\\\',\n",
    "    r'.\\\\data2\\\\Figure_normal_legs_resized\\\\',\n",
    "    r'.\\\\data2\\\\FigureWheels_resized\\\\',\n",
    "    r'.\\\\data2\\\\Head_resized\\\\'\n",
    "]\n",
    "\n",
    "# loading the data for a single image label\n",
    "def load_parts(folder, min_num_images):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    \n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        \n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "X_datasets = list()\n",
    "Y_datasets = list()\n",
    "\n",
    "for idx in range(len(folders_resized)):\n",
    "    folder = folders_resized[idx] \n",
    "    X_datasets.append(load_parts(folder, 100))\n",
    "    labels = np.zeros((X_datasets[-1].shape[0],len(folders_resized)))\n",
    "    labels[:,idx] = 1\n",
    "    Y_datasets.append(labels)\n",
    "\n",
    "X_datasets2 = np.concatenate(X_datasets)\n",
    "Y_datasets2 = np.concatenate(Y_datasets)\n",
    "print(\"Total samples number:\", X_datasets2.shape)\n",
    "\n",
    "X_trains,X_tests,Y_trains,Y_tests = train_test_split(X_datasets2,Y_datasets2,test_size=0.25)\n",
    "print(\"Samples for tests:\", Y_tests.shape[0])\n",
    "print(\"Samples for trains:\", Y_trains.shape[0])\n",
    "\n",
    "plt.imshow(X_tests[0],cmap='gray')\n",
    "\n",
    "\n",
    "\n",
    "# TASK: BUILD THE MODEL NETWORKS, COMPILE & FIT IT, WRITE DOWN THE ACCURACY/ERROR OF THE MODEL\n",
    "# create the model networks with Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Reshape,LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "import numpy\n",
    "\n",
    "# callbacks are a set of rules that will stop the model fitting from being too long or infinite\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.00001, verbose=1),\n",
    "    ModelCheckpoint(filepath='./weights_test.net', verbose=1, save_best_only=True),\n",
    "]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "input_dim = X_trains[0].shape[0]*X_trains[0].shape[1]\n",
    "print((X_trains[0].shape[0], X_trains[0].shape[1]))\n",
    "print(Y_trains[0].shape[0])\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((input_dim,), input_shape=(X_trains[0].shape[0],X_trains[0].shape[1])))\n",
    "model.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(int(input_dim/2), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(Y_trains[0].shape[0], kernel_initializer = 'uniform', name = \"output\"))\n",
    "\n",
    "model.add(Activation('softmax', name = \"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "sto_grad_descent = SGD(lr = 0.2, decay = 0.000001, momentum = 0.9, nesterov = True) #1e-6\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sto_grad_descent, metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_trains, Y_trains, epochs = 10, batch_size = 10, verbose = 2, validation_split = 0.25, callbacks = callbacks)\n",
    "\n",
    "# calculate predictions\n",
    "results = model.evaluate(X_tests, Y_tests, batch_size = 32, verbose = 1, sample_weight = None)\n",
    "\n",
    "# round predictions\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
