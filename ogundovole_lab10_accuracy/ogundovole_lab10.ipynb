{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\\\notMNIST_small\\\\A\\\\\n",
      "Could not read: .\\\\notMNIST_small\\\\A\\\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file '.\\\\\\\\notMNIST_small\\\\\\\\A\\\\\\\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      ".\\\\notMNIST_small\\\\B\\\\\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      ".\\\\notMNIST_small\\\\C\\\\\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      ".\\\\notMNIST_small\\\\D\\\\\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      ".\\\\notMNIST_small\\\\E\\\\\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      ".\\\\notMNIST_small\\\\F\\\\\n",
      "Could not read: .\\\\notMNIST_small\\\\F\\\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file '.\\\\\\\\notMNIST_small\\\\\\\\F\\\\\\\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      ".\\\\notMNIST_small\\\\G\\\\\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      ".\\\\notMNIST_small\\\\H\\\\\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      ".\\\\notMNIST_small\\\\I\\\\\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      ".\\\\notMNIST_small\\\\J\\\\\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n",
      "Total samples number: (18724, 28, 28)\n",
      "Samples for tests: 4681\n",
      "Samples for trains: 14043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b223a0eeb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b21909de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "% matplotlib inline\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "folders = [\n",
    "    r'.\\\\notMNIST_small\\\\A\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\B\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\C\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\D\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\E\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\F\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\G\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\H\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\I\\\\',\n",
    "    r'.\\\\notMNIST_small\\\\J\\\\'\n",
    "]\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  # Load the data for a single letter label.\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    \n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        \n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "X_datasets = list()\n",
    "Y_datasets = list()\n",
    "\n",
    "for idx in range(len(folders)):\n",
    "    folder = folders[idx] \n",
    "    X_datasets.append(load_letter(folder, 1800))\n",
    "    labels = np.zeros((X_datasets[-1].shape[0],len(folders)))\n",
    "    labels[:,idx] = 1\n",
    "    Y_datasets.append(labels)\n",
    "\n",
    "\n",
    "X_datasets2 = np.concatenate(X_datasets)\n",
    "Y_datasets2 = np.concatenate(Y_datasets)\n",
    "print(\"Total samples number:\",X_datasets2.shape)\n",
    "\n",
    "X_trains,X_tests,Y_trains,Y_tests = train_test_split(X_datasets2,Y_datasets2,test_size=0.25)\n",
    "print(\"Samples for tests:\",Y_tests.shape[0])\n",
    "print(\"Samples for trains:\",Y_trains.shape[0])\n",
    "\n",
    "plt.imshow(X_tests[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_21 (Reshape)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 392)               307720    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                3930      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 927,090\n",
      "Trainable params: 927,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10532 samples, validate on 3511 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.06852, saving model to ./weights.net\n",
      " - 10s - loss: 0.0938 - acc: 0.9700 - val_loss: 0.0685 - val_acc: 0.9789\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 9s - loss: 0.0757 - acc: 0.9766 - val_loss: 0.0899 - val_acc: 0.9759\n",
      "Epoch 00002: early stopping\n",
      "4681/4681 [==============================] - 0s 75us/step\n",
      "[0.08138996370007813, 0.9765221243535217]\n"
     ]
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Reshape,LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "import numpy\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.00001, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(filepath='./weights.net', verbose=1, save_best_only=True),\n",
    "]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "input_dim = X_trains[0].shape[0]*X_trains[0].shape[1]\n",
    "print((X_trains[0].shape[0], X_trains[0].shape[1]))\n",
    "print(Y_trains[0].shape[0])\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((input_dim,), input_shape=(X_trains[0].shape[0],X_trains[0].shape[1])))\n",
    "model.add(Dense(input_dim, input_shape = (input_dim,), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(int(input_dim/2), kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(Y_trains[0].shape[0], kernel_initializer = 'uniform', name = \"output\"))\n",
    "\n",
    "model.add(Activation('softmax', name = \"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "sto_grad_descent = SGD(lr = 0.2, decay = 0.000001, momentum = 0.9, nesterov = True) #1e-6\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = sto_grad_descent, metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_trains, Y_trains, epochs = 10, batch_size = 10, verbose = 2, validation_split = 0.25, callbacks = callbacks)\n",
    "\n",
    "# calculate predictions\n",
    "results = model.evaluate(X_tests, Y_tests, batch_size = 32, verbose = 1, sample_weight = None)\n",
    "\n",
    "# round predictions\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
